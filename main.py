# -*- coding: utf-8 -*-
"""Multi_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xxisHBsCU8J8Ajr7vaLFam0mMVspCdX5
"""

# colab用
"""
import shutil
import os

# 1. Google Driveをマウント
from google.colab import drive
drive.mount('/content/drive')

# 2. データをGoogle DriveからColabのローカルストレージにコピー

# ディレクトリパスを指定
directory = '/content/data'

# ディレクトリが存在しない場合に作成
if not os.path.exists(directory):
    os.makedirs(directory)
    print(f"Directory {directory} created.")
else:
    print(f"Directory {directory} already exists.")


shutil.copyfile('/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/data/train.json', '/content/data/train.json')
shutil.copyfile('/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/data/valid.json', '/content/data/valid.json')
!unzip /content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/train.zip -d /content/data
!unzip /content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/valid.zip -d /content/data


# CLIP
!pip install git+https://github.com/openai/CLIP.git

# torchinfo
!pip install torchinfo
"""

import re
import random
import time
from statistics import mode
import os

from PIL import Image
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchvision import transforms
from tqdm import tqdm
import clip

import matplotlib.pyplot as plt

from torch.cuda.amp import autocast, GradScaler
from torchinfo import summary
#import inflect

# for Vilt
from transformers import ViltProcessor, ViltForQuestionAnswering, ViltConfig
from torchvision.models import vit_b_16
from torchvision.models import resnet50

# for MMBT
from transformers import BertTokenizer, BertModel
from torchvision.models import resnet152
from torchvision.models import ResNet152_Weights

# for CLIP
import clip
from torch.cuda.amp import autocast

import gc


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# GPUチェック
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
print(torch.cuda.device_count()) # 使用可能gpu数
print(torch.cuda.get_device_name()) # gpu名

"""### EDA"""

# configの設定
class InferenceConfig:
    """
    model: embedding_size
    'RN50': 1024,
    'RN101': 512,
    'RN50x4': 640,
    'RN50x16': 768,
    'RN50x64': 448,
    'ViT-B/32': 512,
    'ViT-B/16': 512,
    'ViT-L/14': 768,
    'ViT-L/14@336px': 768
    """
    model: str = 'ViT-B/32' # for CLIP
    n_classes: int = 6392
    train_image_dir: str = "./data/train"
    train_df_dir: str = "./data/train.json"
    valid_image_dir: str = "./data/valid"
    valid_df_dir: str = "./data/valid.json"
    class_mapping: str = "class_mapping.csv"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    aux_num_class = 5
    bert_model = 'bert-base-uncased'
config = InferenceConfig()

"""### クラス・関数定義

訓練済みモデル
"""

# ViLT
processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
vilt_model = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
vilt_config  = ViltConfig.from_pretrained("dandelin/vilt-b32-finetuned-vqa")


# CLIP
clip_model, preprocess = clip.load(name=config.model, device=config.device)
# エンコーダ―へ入力する画像サイズを確認
input_resolution = clip_model.visual.input_resolution
print(f"Input resolution for {config.model} model: {input_resolution}")

"""画像の前処理定義"""

try:
    from torchvision.transforms import InterpolationMode
    BICUBIC = InterpolationMode.BICUBIC
except ImportError:
    BICUBIC = Image.BICUBIC


# データ拡張の定義
transform_resize = transforms.Compose([
    transforms.Resize((224, 224), interpolation=BICUBIC),
    transforms.ToTensor(),

])

transform_rotate_90 = transforms.Compose([
    transforms.Resize((224, 224), interpolation=BICUBIC),
    transforms.RandomRotation((90, 90)),
    transforms.ToTensor(),

])

transform_rotate_270 = transforms.Compose([
    transforms.Resize((224, 224), interpolation=BICUBIC),
    transforms.RandomRotation((270, 270)),
    transforms.ToTensor(),

])



# for Test
test_transform = transforms.Compose([
    transforms.Resize((224, 224), interpolation=BICUBIC),
    transforms.ToTensor(),

])

test_transform_rotate_90 = transforms.Compose([
    transforms.Resize((224, 224), interpolation=BICUBIC),
    transforms.RandomRotation((90, 90)),
    transforms.ToTensor(),

])

test_transform_rotate_270 = transforms.Compose([
    transforms.Resize((224, 224), interpolation=BICUBIC),
    transforms.RandomRotation((270, 270)),
    transforms.ToTensor(),

])



# for CLIP
n_px = clip_model.visual.input_resolution
preprocess =  transforms.Compose([
        transforms.Resize((n_px, n_px), interpolation=BICUBIC),
        transforms.ToTensor(),
        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])

preprocess_rotate_90 = transforms.Compose([
        transforms.Resize((n_px, n_px), interpolation=BICUBIC),
        transforms.RandomRotation(degrees=(90, 90)),  # 90度回転
        transforms.ToTensor(),
        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])

preprocess_rotate_270 = transforms.Compose([
        transforms.Resize((n_px, n_px), interpolation=BICUBIC),
        transforms.RandomRotation(degrees=(270, 270)),  # 90度回転
        transforms.ToTensor(),
        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])



# Clip for test
test_preprocess = transforms.Compose([
    transforms.Resize((n_px, n_px), interpolation=BICUBIC),
    transforms.ToTensor(),

])

test_preprocess_rotate_90 = transforms.Compose([
    transforms.Resize((n_px, n_px), interpolation=BICUBIC),
    transforms.RandomRotation((90, 90)),
    transforms.ToTensor(),

])

test_preprocess_rotate_270 = transforms.Compose([
    transforms.Resize((n_px, n_px), interpolation=BICUBIC),
    transforms.RandomRotation((270, 270)),
    transforms.ToTensor(),

])


preprocess_set = [preprocess, preprocess_rotate_90, preprocess_rotate_270]
transform_set = [transform_resize, transform_rotate_90, transform_rotate_270]

"""文章の前処理定義"""

# questionに対する前処理
# 数詞を数字に変換する辞書
def text2int(textnum, numwords={}):
    if not numwords:
        units = [
            "zero", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten",
            "eleven", "twelve", "thirteen", "fourteen", "fifteen", "sixteen", "seventeen", "eighteen", "nineteen",
        ]
        tens = ["", "", "twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty", "ninety"]
        scales = ["hundred", "thousand", "million", "billion"]

        numwords["and"] = (1, 0)
        for idx, word in enumerate(units): numwords[word] = (1, idx)
        for idx, word in enumerate(tens): numwords[word] = (1, idx * 10)
        for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)

    current = result = 0
    for word in textnum.split():
        if word not in numwords:
            raise Exception("Illegal word: " + word)

        scale, increment = numwords[word]
        current = current * scale + increment
        if scale > 100:
            result += current
            current = 0

    return result + current

def replace_num_words(text):
    num_word_to_digit = {
        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',
        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',
        'ten': '10', 'eleven': '11', 'twelve': '12', 'thirteen': '13', 'fourteen': '14',
        'fifteen': '15', 'sixteen': '16', 'seventeen': '17', 'eighteen': '18', 'nineteen': '19',
        'twenty': '20', 'thirty': '30', 'forty': '40', 'fifty': '50', 'sixty': '60',
        'seventy': '70', 'eighty': '80', 'ninety': '90', 'hundred': '100', 'thousand': '1000',
        'million': '1000000', 'billion': '1000000000'
    }

    def text2int_full(text):
        numwords = {}
        units = [
            "zero", "one", "two", "three", "four", "five", "six", "seven", "eight", "nine", "ten",
            "eleven", "twelve", "thirteen", "fourteen", "fifteen", "sixteen", "seventeen", "eighteen", "nineteen",
        ]
        tens = ["", "", "twenty", "thirty", "forty", "fifty", "sixty", "seventy", "eighty", "ninety"]
        scales = ["hundred", "thousand", "million", "billion"]

        numwords["and"] = (1, 0)
        for idx, word in enumerate(units): numwords[word] = (1, idx)
        for idx, word in enumerate(tens): numwords[word] = (1, idx * 10)
        for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)

        current = result = 0
        for word in text.split():
            if word not in numwords:
                raise Exception("Illegal word: " + word)

            scale, increment = numwords[word]
            current = current * scale + increment
            if scale > 100:
                result += current
                current = 0

        return result + current

    num_word_pattern = re.compile(r'\b(?:' + '|'.join(re.escape(word) for word in num_word_to_digit.keys()) + r')\b', re.IGNORECASE)

    def replace(match):
        try:
            return str(text2int_full(match.group(0).lower())) + ' '
        except Exception as e:
            return match.group(0) + ' '

    num_pattern = re.compile(r'((?:\b(?:one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand|million|billion)\b\s*)+)')
    text = num_pattern.sub(replace, text)

    return text

def preprocess_question(text):
    text = text.strip()  # 文頭・文末のスペースを削除

    # 数詞を数字に変換
    text = replace_num_words(text)

    # カンマ、ピリオド、クエスチョンマークの前後にスペースを追加（小数点を除く）
    text = re.sub(r'(?<=\S)([,.?])(?=\S)', r' \1 ', text)
    text = re.sub(r'(?<=\S)([．，？])(?=\S)', r' \1 ', text)

    # カンマ、ピリオド、クエスチョンマークの直前にスペースを追加（小数点を除く）
    text = re.sub(r'(?<=\S)([,.?])', r' \1', text)
    text = re.sub(r'(?<=\S)([．，？])', r' \1', text)

    # 小数点の前後が数値の場合はスペースを削除
    text = re.sub(r'(\d) \. (\d)', r'\1.\2', text)

    # 短縮形のカンマの追加
    contractions = {
        "dont": "don't", "isnt": "isn't", "arent": "aren't", "wont": "won't",
        "cant": "can't", "wouldnt": "wouldn't", "couldnt": "couldn't"
    }
    for contraction, correct in contractions.items():
        text = text.replace(contraction, correct)

    # 連続するスペースを1つに変換
    text = re.sub(r'\s+', ' ', text).strip()

    return text


# answer用処理
def process_text(text):
    # lowercase
    # 単語をすべて小文字に変換
    text = text.lower()

    # 数詞を数字に変換
    num_word_to_digit = {
        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',
        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',
        'ten': '10'
    }
    for word, digit in num_word_to_digit.items():
        text = text.replace(word, digit)

    # 小数点のピリオドを削除
    text = re.sub(r'(?<!\d)\.(?!\d)', '', text)

    # 冠詞の削除
    text = re.sub(r'\b(a|an|the)\b', '', text)

    # 短縮形のカンマの追加
    contractions = {
        "dont": "don't", "isnt": "isn't", "arent": "aren't", "wont": "won't",
        "cant": "can't", "wouldnt": "wouldn't", "couldnt": "couldn't"
    }
    for contraction, correct in contractions.items():
        text = text.replace(contraction, correct)

    # 句読点をスペースに変換
    text = re.sub(r"[^\w\s':]", ' ', text)

    # 句読点をスペースに変換
    text = re.sub(r'\s+,', ',', text)

    # 連続するスペースを1つに変換
    text = re.sub(r'\s+', ' ', text).strip()

    return text

"""## EDA"""

# データフレームの定義
df = pd.read_json(config.train_df_dir)

# mode_answerを抽出
mode_answers = []
for idx in range(len(df)):
    ans_list = []
    for answer in df["answers"][idx]:
        ans_list.append(process_text(answer["answer"]))
    mode_answers.append(mode(ans_list))
df.insert(2, "mode_answer", mode_answers)


# mode_answer_list
unique_counts = df["mode_answer"].value_counts()
unique_counts_df = unique_counts.reset_index()
unique_counts_df.columns = ['mode_answer', 'count']

# class_mapとの比較
class_map = pd.read_csv(config.class_mapping)
class_map_list = class_map["answer"].tolist()
not_in_class_map = []

for word, count in unique_counts.items():
    if word not in class_map_list:
        not_in_class_map.append({'word': word, 'count': count})

not_in_class_map_df = pd.DataFrame(not_in_class_map)

not_in_class_map_df.head(20)
len(unique_counts)


# 色のリストを定義
color_list = ['red', 'blue', 'green', 'yellow', 'black', 'white', 'orange', 'purple', 'pink', 'brown', 'grey', "gold"]
yes_no_list = ["yes", "no"]
number_list = ["0", "1" ,"2", "3", "4", "5", "6", "7", "8", "9"]
serch_list = number_list
# 色を表現する単語のみを抜き出す
counts = unique_counts_df[unique_counts_df['mode_answer'].isin(serch_list)]
counts


# マルチタスク用のデータフレームを作る
# unanswerableカラム
# unanswerableの場合は1を，その他の場合は0を返す．
df["unans_flg"] = df["mode_answer"].apply(lambda x: 0 if x=="unanswerable" else 1)
# カラムの順序を変更して、"unans_flg"を最後から2番目の位置に移動
cols = df.columns.tolist()
unans_flg_index = cols.index('unans_flg')
cols.insert(len(cols) - 2, cols.pop(unans_flg_index))
df = df[cols]


# yes/no カラム
df['yes/no'] = df['mode_answer'].apply(lambda x: 1 if x == 'yes' else (2 if x == 'no' else 0))
# カラムの順序を変更して、'yes/no'を最後から2番目の位置に移動
cols = df.columns.tolist()
unans_flg_index = cols.index('yes/no')
cols.insert(len(cols) - 2, cols.pop(unans_flg_index))
df = df[cols]

# colorカラム
# "white":1. "grey":2, "black":3, "blue":4, "red":5, "'pink', 'green', 'purple', 'yellow', 'orange', 'gold'":6,
color_mapping = {
    'white': 1, 'grey': 2, 'black': 3, 'blue': 4, 'red': 5,
    'pink': 6, 'green': 6, 'purple': 6, 'yellow': 6, 'orange': 6, 'gold': 6
}

df['color_flg'] = df['mode_answer'].apply(lambda x: color_mapping.get(x, 0))
# カラムの順序を変更して、'color_flg'を最後から2番目の位置に移動
cols = df.columns.tolist()
unans_flg_index = cols.index('color_flg')
cols.insert(len(cols) - 2, cols.pop(unans_flg_index))
df = df[cols]

# 数値データであるかを判定し、新しいカラム"is_numeric_answer"に格納
def is_numeric(s):
    try:
        float(s)
        return True
    except ValueError:
        return False

df['number_flg'] = df['mode_answer'].apply(lambda x: 1 if is_numeric(x) else 0)
# カラムの順序を変更して、'number_flg'を最後から2番目の位置に移動
cols = df.columns.tolist()
unans_flg_index = cols.index('number_flg')
cols.insert(len(cols) - 2, cols.pop(unans_flg_index))
df = df[cols]

df.head(10)

print("unans: ", df["unans_flg"].sum())
print("yes/no: ", len(df[df["yes/no"]==1]) + len(df[df["yes/no"]==2]))
print("  yes:  ", len(df[df["yes/no"]==1]))
print("  no:  ", len(df[df["yes/no"]==2]))
print("color:  ", len(df[df["color_flg"]!=0]))
print("number: ", df["number_flg"].sum())

# 4分類用
# 線形層の最後の出力次元も分類数に対応させる
answer_labels = {
    "unanswerable": 0,
    "no": 1,
    "yes": 2,
    "color": 3,
    "other": 4
}

colors = ["white", "grey", "black", "blue", "red", "brown", "pink", "green", "purple", "yellow", "orange", "gold"]

# ラベル付け関数
def map_answer_to_label(answer):
    answer = answer.lower()
    if answer in answer_labels:
        return answer_labels[answer]
    elif answer in ["yes", "no"]:
        return answer_labels[answer]
    elif any(color in answer for color in colors):
        return answer_labels["color"]
    else:
        return answer_labels["other"]

# データフレームに新しいラベルカラムを作成
df['answer_label'] = df['mode_answer'].apply(map_answer_to_label)

# 新しいラベルカラムを4番目に挿入
df.insert(3, 'answer_label', df.pop('answer_label'))

print("データサイズ", len(df))
df.head(10)

image_paths_aug = []
questions_aug = []
mode_answer_aug = []
labels_aug = []
unans_flg_aug = []
yes_no_aug = []
color_flg_aug = []
number_flg_aug = []
answers_aug = []
transforms_aug = []
preprocess_aug = []

# データ拡張によるサンプル増加
for i in range(len(df)):
    for k in range(len(transform_set)):
        image_paths_aug.append(df['image'][i])
        questions_aug.append(df['question'][i])
        mode_answer_aug.append(df["mode_answer"][i])
        labels_aug.append(df['answer_label'][i])
        unans_flg_aug.append(df["unans_flg"][i])
        yes_no_aug.append(df["yes/no"][i])
        color_flg_aug.append(df["color_flg"][i])
        number_flg_aug.append(df["number_flg"][i])
        answers_aug.append(df["answers"][i])
        transforms_aug.append(transform_set[k])
        preprocess_aug.append(preprocess_set[k])

# 拡張後のデータフレームを作成
aug_df = pd.DataFrame({
    'image': image_paths_aug,
    'question': questions_aug,
    'mode_answer':mode_answer_aug,
    'answer_label': labels_aug,
    'unans_flg':unans_flg_aug,
    'yes_no':yes_no_aug,
    'color_flg':color_flg_aug,
    'number_flg':number_flg_aug,
    'answers': answers_aug,
})
print("拡張後データサイズ", len(aug_df))
aug_df.head(20)

class Train_Dataset(torch.utils.data.Dataset):
    def __init__(self, df, image_dir, class_map_path, transforms=None, preprocesses=None, answer=True):
        self.transforms = transforms  # 画像の前処理
        self.image_dir = image_dir  # 画像ファイルのディレクトリ
        self.preprocesses = preprocesses #CLIPの画像に対する前処理
        self.df = df  # 拡張済みdf
        self.answer = answer

        self.answer2idx = {}
        self.idx2answer = {}
        self.ClassMap2idx = {}
        self.idx2ClassMap = {}
        # 回答辞書を作成(訓練データ・testデータ同時に作成)
        if self.answer:

            # class_mapに含まれる回答を辞書に追加
            class_mapping_df = pd.read_csv(class_map_path)
            class_list = class_mapping_df['answer'].tolist()
            for word in class_list:
                word = process_text(word)  # answerに関しては，単語ごとにsplitしない
                                            # 各回答を1つの単語として登録
                if word not in self.ClassMap2idx:
                    self.ClassMap2idx[word] = len(self.ClassMap2idx) # 回答の選択肢用
                    self.answer2idx[word] = len(self.answer2idx)     # 全回答パターン記録用

            # answersの最頻回答を辞書に追加
            for answers in self.df["answers"]:
                # 10人の回答をリストで取得
                answers_list = [process_text(answer["answer"]) for answer in answers]
                mode_word = mode(answers_list) # 最頻回答を取得

                if mode_word not in self.ClassMap2idx:
                    self.ClassMap2idx[mode_word] = len(self.ClassMap2idx)  # 回答の選択肢用
                    self.answer2idx[mode_word] = len(self.answer2idx)      # 全回答パターン記録


            # 全回答をanser2idx辞書に追加
            for answers in self.df["answers"]:
                for answer in answers:
                    word = answer["answer"]
                    word = process_text(word)  # answerに関しては，単語ごとにsplitしない
                                               # 各回答を1つの単語として登録
                    if word not in self.answer2idx:
                        self.answer2idx[word] = len(self.answer2idx)

            """
            以上の操作で，
            class_mapの単語リスト及び訓練データの最頻回答までは，
            word2idx ⇔ idx2word のidが一致する．
            評価指標に必要な他のanswerのidはanswer2idxにのみ追加．
            こうすることで，推論時の回答選択肢を減少させつつ，
            評価指標の作成も可能になる
            """


            self.idx2ClassMap = {v: k for k, v in self.ClassMap2idx.items()}  # 逆変換用の辞書(answer)
            #self.idx2Mode_Answer = {v: k for k, v in self.Mode_Answer2idx.items()}  # 逆変換用の辞書(answer)
            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)
            print("回答パターン： ", len(self.ClassMap2idx))
            #print("最頻回答パターン： ", len(self.Mode_Answer2idx))
            print("全回答リスト： ", len(self.answer2idx))
            print("データサイズ: ", len(self.df) )

    def update_dict(self, dataset):
        """
        検証用データ，テストデータの辞書を訓練データの辞書に更新する．

        Parameters
        ----------
        dataset : Dataset
            訓練データのDataset
        """
        self.answer2idx = dataset.answer2idx
        self.ClassMap2idx = dataset.ClassMap2idx
        self.idx2answer = dataset.idx2answer
        self.idx2ClassMap = dataset.idx2ClassMap

    def __getitem__(self, idx):
        """
        対応するidxのデータ（画像，質問，回答）を取得．

        Parameters
        ----------
        idx : int
            取得するデータのインデックス

        Returns
        -------
        image : torch.Tensor  (C, H, W)
            画像データ
        question : torch.Tensor  (vocab_size)
            質問文をone-hot表現に変換したもの
        answers : torch.Tensor  (n_answer)
            10人の回答者の回答のid
        mode_answer_idx : torch.Tensor  (1)
            10人の回答者の回答の中で最頻値の回答のid
        """

        # 画像：
        image = Image.open(f"{self.image_dir}/{self.df['image'][idx]}")
        if self.preprocesses is None:
          # transformの適用
          transform = self.transforms[idx % len(self.transforms)]
          image = transform(image)
        else:
          preprocess = self.preprocesses[idx % len(self.preprocesses)]
          image = preprocess(image)
        #print(self.df["question"][idx])

        # 質問文の前処理
        question = preprocess_question(self.df["question"][idx]) #questionを変換．tupple化

        labels = self.df["answer_label"][idx] #正解ラベルの取得
        unans_label = self.df["unans_flg"][idx] # unans_flgの取得
        yesno_label = self.df["yes_no"][idx] # yes/noの取得
        color_label = self.df["color_flg"][idx] # color_flgの取得
        number_label = self.df["number_flg"][idx] # number_flgの取得


        if self.answer:
            answers = [self.answer2idx[process_text(answer["answer"])] for answer in self.df["answers"][idx]]
            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）

            return image, question, labels, torch.Tensor(answers), int(mode_answer_idx), unans_label, yesno_label, color_label
        else:
            return image, question, labels, unans_label, yesno_label, color_label

    def __len__(self):
        return len(self.df)

class Test_Dataset(torch.utils.data.Dataset):
    def __init__(self, df_path, image_dir, class_map_path, transform=None, preprocess=None):
        self.transforms = transform  # 画像の前処理
        self.image_dir = image_dir  # 画像ファイルのディレクトリ
        self.preprocess = preprocess #CLIPの画像に対する前処理
        self.df = pd.read_json(df_path)  # vali_dfの読み込み

        self.answer2idx = {}
        self.idx2answer = {}
        self.ClassMap2idx = {}
        self.idx2ClassMap = {}

    def update_dict(self, dataset):
        """
        検証用データ，テストデータの辞書を訓練データの辞書に更新する．

        Parameters
        ----------
        dataset : Dataset
            訓練データのDataset
        """
        self.answer2idx = dataset.answer2idx
        self.ClassMap2idx = dataset.ClassMap2idx
        self.idx2answer = dataset.idx2answer
        self.idx2ClassMap = dataset.idx2ClassMap

    def __getitem__(self, idx):
        """
        対応するidxのデータ（画像，質問，回答）を取得．

        Parameters
        ----------
        idx : int
            取得するデータのインデックス

        Returns
        -------
        image : torch.Tensor  (C, H, W)
            画像データ
        question : torch.Tensor  (vocab_size)
            質問文をone-hot表現に変換したもの
        answers : torch.Tensor  (n_answer)
            10人の回答者の回答のid
        mode_answer_idx : torch.Tensor  (1)
            10人の回答者の回答の中で最頻値の回答のid
        """

        # 画像：CLIPによる前処理
        image = Image.open(f"{self.image_dir}/{self.df['image'][idx]}")
        if self.preprocess is None:
          image = self.transforms(image)
        else:
          image = self.preprocess(image)

        # 質問文の前処理
        question = preprocess_question(self.df["question"][idx]) #questionを変換．

        return image, question

    def __len__(self):
        return len(self.df)


# 評価指標の実装
def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):
    total_acc = 0.

    for pred, answers in zip(batch_pred, batch_answers):
        acc = 0.
        for i in range(len(answers)):
            num_match = 0
            for j in range(len(answers)):
                if i == j:
                    continue
                if pred == answers[j]:
                    num_match += 1
            acc += min(num_match / 3, 1)
        total_acc += acc / 10

    return total_acc / len(batch_pred)

"""## メイン分類器の作成"""

# 重みの初期化方法
def initialize_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            nn.init.zeros_(m.bias)
    elif isinstance(m, nn.BatchNorm1d):
        nn.init.ones_(m.weight)
        nn.init.zeros_(m.bias)

#Influence-balanced loss と Focal Loss
def ib_focal_loss(input_values, ib, gamma):
    """Computes the ib focal loss"""
    p = torch.exp(-input_values)
    loss = (1 - p) ** gamma * input_values * ib
    return loss.mean()

class IB_FocalLoss(nn.Module):
    def __init__(self, config, weight=None, alpha=1000., gamma=1.):
        super(IB_FocalLoss, self).__init__()
        assert alpha > 0
        self.alpha = alpha
        self.epsilon = 0.001
        self.weight = weight
        self.gamma = gamma
        self.config = config

    def forward(self, input, target, features, n_classes):
        grads = torch.sum(torch.abs(F.softmax(input, dim=1) - F.one_hot(target, n_classes)),1) # N * 1
        ib = grads*(features.reshape(-1))
        ib = self.alpha / (ib + self.epsilon)
        return ib_focal_loss(F.cross_entropy(input, target, reduction='none', weight=self.weight), ib, self.gamma)

"""### CLIP_MultiTaskModel

"""

class MultiTask_Clf(nn.Module):
    def __init__(self, config):
        super().__init__()

        embedding_size = {'RN50': 1024,
                          'RN101': 512,
                          'RN50x4': 640,
                          'RN50x16': 768,
                          'RN50x64': 1024,
                          'ViT-B/32': 512,
                          'ViT-B/16': 512,
                          'ViT-L/14': 768,
                          'ViT-L/14@336px': 768}

        # Main_head
        # image_featureとquestion_featureを入れるのでembedding_sizeの2倍
        self.ln1 = torch.nn.LayerNorm(embedding_size[config.model]*2)
        self.dp1 = torch.nn.Dropout(0.5)
        self.fc1 = torch.nn.Linear(embedding_size[config.model] * 2, 512)

        self.ln2 = torch.nn.LayerNorm(512)
        self.dp2 = torch.nn.Dropout(0.5)

        self.fc2 = torch.nn.Linear(512, 256)

        self.bn3 = torch.nn.BatchNorm1d(256)
        self.fc3 = torch.nn.Linear(256, config.n_classes)

        # Mask
        self.answerlabel_head = nn.Sequential(
            nn.Linear(512, 5),
        )

        self.mask_head = nn.Sequential(
            nn.Linear(5, 1),
            nn.Sigmoid()
        )

        # unans_head
        self.unans_head = nn.Sequential(
            nn.Linear(embedding_size[config.model]*2, 1),
            nn.Sigmoid()
        )

        # yesno_head
        self.yesno_head = nn.Sequential(
            nn.Linear(embedding_size[config.model]*2, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, 3)
        )

        # color_head
        self.color_head = nn.Sequential(
            nn.Linear(embedding_size[config.model]*2, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, 7)
        )

        """
        # number_head
        self.number_head = nn.Sequential(
            nn.Linear(embedding_size[config.model]*2, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, 2)
        )
        """
    def forward(self, img_features, text_features):
        # Main
        xc = torch.cat((img_features, text_features), dim=-1)
        x = self.ln1(xc)
        x = self.dp1(x)
        x = self.fc1(xc)

        label_feats = x
        # マスク機構
        answer_label = self.answerlabel_head(label_feats) # embedding_size -> 5 answer_label
        mask = self.mask_head(answer_label) # 5 -> 6392

        x = F.relu(x)
        x = self.ln2(x)
        x = self.dp2(x)
        x = self.fc2(x)
        x = F.relu(x)
        feats = self.bn3(x)
        vqa = self.fc3(feats)

        label_feats = torch.sum(torch.abs(label_feats), 1).reshape(-1, 1)
        feats = torch.sum(torch.abs(feats), 1).reshape(-1, 1)
        output = vqa * mask

        # sub task
        # unans
        unans_logits = self.unans_head(xc).squeeze(-1)


        # yes/no
        yesno_logits = self.yesno_head(xc)

        # color
        color_logits = self.color_head(xc)

        """
        # number
        number_logits = self.number_head(xc)
        """
        # 最終層の入力特徴量featsはIBLossの計算のために使う
        return output, feats, answer_label, label_feats, unans_logits, yesno_logits, color_logits

model = MultiTask_Clf(config)
summary(model)

#unans_label, yesno_label, color_label, number_label

# 学習
def train(model, clip_model, dataloader, optimizer, criterion, device, IBFLoss=False):
    model.train()

    total_loss = 0
    total_acc = 0
    simple_acc = 0
    start = time.time()

    # sub_criterion
    unans_criterion = nn.BCELoss()
    yesno_criterion = nn.CrossEntropyLoss()
    color_criterion = nn.CrossEntropyLoss()
    number_criterion = nn.CrossEntropyLoss()
    kl_criterion = nn.KLDivLoss(reduction="batchmean")

    for image, question, label, answers, mode_answer, unans_label, yesno_label, color_label in dataloader:
        optimizer.zero_grad()
        image = image.to(device)
        text = clip.tokenize(question, truncate=True).to(device) # 質問文をIDに変換
        # truncateは質問文が長すぎた際に，途中で打ち切るかどうか
        # Trueにすることで，tokenの長さが固定される．
        label = label.to(device)
        answers = answers.to(device)
        mode_answer = mode_answer.to(device)
        unans_label = unans_label.float().to(device)
        yesno_label = yesno_label.to(device)
        color_label = color_label.to(device)
        #number_label = number_label.to(device)



        # embedding
        with torch.no_grad():
          text_features = clip_model.encode_text(text)
          # text_features.shape  [batch_size, 512]
          image_features = clip_model.encode_image(image)
          # image_features.shape  [batch_size, 512]


        # 推論
        with autocast():
            pred, features, pred_label, label_features, unans_logits, yesno_logits, color_logits = model(image_features, text_features)


        pred = pred.float()
        features = features.float()
        unans_logits = unans_logits.float()
        yesno_logits = yesno_logits.float()
        color_logits = color_logits.float()
        #number_logits = number_logits.float()

        # for KLDiv
        log_preds = F.log_softmax(pred, dim=1)
        one_hot_targets = F.one_hot(mode_answer, num_classes=config.n_classes).float()

        if IBFLoss :
          loss_main = criterion(pred, mode_answer, features, config.n_classes) # Influence-BalancedFocalLoss
          loss_label = criterion(pred_label, label, label_features, 5) # Influence-BalancedFocalLoss
        else:
          loss_main = criterion(pred, mode_answer) #CrossEntropyLoss
          loss_label = criterion(pred_label, label) #CrossEntropyLoss
        loss_kldiv = kl_criterion(log_preds, one_hot_targets) # KLDivLoss
        loss_unans = unans_criterion(unans_logits, unans_label) # BCELoss
        loss_yesno = yesno_criterion(yesno_logits, yesno_label)
        loss_color = color_criterion(color_logits, color_label)
        #loss_number = number_criterion(number_logits, number_label)

        loss = loss_main + loss_label + loss_unans + loss_yesno + loss_color + loss_kldiv
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy
        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy

    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start

batch_size = 128
model_save = True
submission = True

print("CLIP_Model", config.model)
"""
CLIP用画像前処理
 preprocess:  回転無し
 preprocess_rotate90:  90°回転
 preprocess_rotate180:  180°回転
"""

train_dataset = Train_Dataset(aug_df, image_dir=config.train_image_dir, class_map_path=config.class_mapping, transforms=None, preprocesses=preprocess_aug, answer=True)
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=None, preprocess=test_preprocess)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)

# loaderの中身
# image:       [batch_size, 3, 224, 224]
# question:    sentence
# answers:     [batch_size, 10]
# mode_answer: [batch_size, ]

epochs = 10
model = MultiTask_Clf(config).to(config.device)
model.apply(initialize_weights)
optimizer = torch.optim.Adam(model.parameters(), lr=8e-4, weight_decay=5e-3)
criterion = nn.CrossEntropyLoss()


for epoch in tqdm(range(epochs)):
    train_loss, train_acc, train_simple_acc, train_time = train(model, clip_model, train_loader, optimizer, criterion, config.device, IBFLoss=False)
    print(f"【{epoch + 1}/{epochs}】\n"
              f"train time: {train_time:.2f} [s]\n"
              f"train loss: {train_loss:.4f}\n"
              f"train acc: {train_acc:.4f}\n"
              f"train simple acc: {train_simple_acc:.4f}")

if model_save:
  # モデルの保存
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/model"
  save_path = os.path.join(save_dir, f"model_MultiTask_KL_{epochs}.pth")  # Driveに保存
  torch.save({
              "epoch": epoch,
              "model.state_dict": model.state_dict(),
              "optimizer_state_dict": optimizer.state_dict(),
              "loss": train_loss,
                }, save_path)


if submission:
  model.eval()
  submission = []
  with torch.no_grad():
      for image, question in tqdm(test_loader):
          image = image.to(device)
          text = clip.tokenize(question, truncate=True).to(device) # 質問文をIDに変換

          # embedding
          text_features = clip_model.encode_text(text)
          # text_features.shape  [batch_size, 768]
          image_features = clip_model.encode_image(image)
          # image_features.shape  [batch_size, 768]
          # 推論
          with autocast():
            pred, features, pred_label, label_features, unans_logits, yesno_logits, color_logits = model(image_features, text_features)
          pred = pred.float()
          pred = pred.argmax(1).cpu().item()
          submission.append(pred)

  submission = [train_dataset.idx2answer[id] for id in submission]
  submission = np.array(submission)
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
  save_path = os.path.join(save_dir, "submission_MultiModel_KL.npy") # Driveの保存
  np.save(save_path, submission)


  # データフレームに変換し，CSV保存
  submission_df = pd.DataFrame(submission, columns=["answer"])
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
  save_path = os.path.join(save_dir, "submission_MultiModel_KL.csv") # Driveの保存  ######## モデル保存名注意！！ ########
  submission_df.to_csv(save_path, index=False)

print("done!!")

# 追加学習 IBFLoss
# 追加の学習数を指定
additional_epochs = 10
batch_size = 128
model_save = True
submission = True

train_dataset = Train_Dataset(aug_df, image_dir=config.train_image_dir, class_map_path=config.class_mapping, transforms=None, preprocesses=preprocess_aug, answer=True)
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=None, preprocess=test_preprocess)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)


# モデルの再定義
model = MultiTask_Clf(config).to(config.device)
optimizer = torch.optim.Adam(model.parameters(), lr=8e-4, weight_decay=5e-3)
criterion = IB_FocalLoss(config)

######## 要変更箇所 ########
checkpoint_path = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/model/model_MultiTask_KL_10.pth"

# モデルとオプティマイザの状態を読み込む
checkpoint = torch.load(checkpoint_path)
model.load_state_dict(checkpoint["model.state_dict"])
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
start_epoch = checkpoint['epoch'] + 1
loss = checkpoint['loss']

print(f"Resuming training from epoch {start_epoch} with last recorded loss {loss:.4f}")


# 追加で学習するエポック数を設定
epochs = start_epoch + additional_epochs

# 学習
for epoch in tqdm(range(start_epoch, epochs)):
    train_loss, train_acc, train_simple_acc, train_time = train(model, clip_model, train_loader, optimizer, criterion, config.device, IBFLoss=True)
    print(f"【{epoch + 1}/{epochs}】\n"
              f"train time: {train_time:.2f} [s]\n"
              f"train loss: {train_loss:.4f}\n"
              f"train acc: {train_acc:.4f}\n"
              f"train simple acc: {train_simple_acc:.4f}")

if model_save:
  # モデルの保存
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/model"
  save_path = os.path.join(save_dir, f"model_MultiTask_KL_{epochs}.pth")  # Driveに保存  ######## モデル保存名注意！！ ########
  torch.save({
              "epoch": epoch,
              "model.state_dict": model.state_dict(),
              "optimizer_state_dict": optimizer.state_dict(),
              "loss": train_loss,
                }, save_path)

if submission:
  model.eval()
  submission = []
  with torch.no_grad():
      for image, question in tqdm(test_loader):
          image = image.to(device)
          text = clip.tokenize(question, truncate=True).to(device) # 質問文をIDに変換

            # embedding
          text_features = clip_model.encode_text(text)
          # text_features.shape  [batch_size, 768]
          image_features = clip_model.encode_image(image)
          # image_features.shape  [batch_size, 768]
          # 推論
          with autocast():
            pred, features, pred_label, label_features, unans_logits, yesno_logits, color_logits = model(image_features, text_features)
          pred = pred.float()
          pred = pred.argmax(1).cpu().item()
          submission.append(pred)

  submission = [train_dataset.idx2answer[id] for id in submission]
  submission = np.array(submission)
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
  save_path = os.path.join(save_dir, "submission_MultiModel_KL.npy") # Driveの保存
  np.save(save_path, submission)


  # データフレームに変換し，CSV保存
  submission_df = pd.DataFrame(submission, columns=["answer"])
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
  save_path = os.path.join(save_dir, "submission_MultiModel_KL.csv") # Driveの保存  ######## モデル保存名注意！！ ########
  submission_df.to_csv(save_path, index=False)

print("done!!")

del model
torch.cuda.empty_cache()

"""### MMBT"""

# モデル定義
class ImageBertEmbeddings(nn.Module):
    def __init__(self, config, embeddings):
        super().__init__()
        self.image_embeddings = nn.Linear(2048, 768)
        # Resnetの出力をBERTに入力できるよう，次元を調整

        self.position_embeddings = (
            embeddings.position_embeddings
        )  # 入力トークンの位置情報を把握するためのベクトル(最大文字数の種類分のベクトル表現)
        self.token_type_embeddings = (
            embeddings.token_type_embeddings
        )  # 各単語が1文目なのか2文目なのかn文目なのかを示す位置ベクトル

        self.ln = nn.LayerNorm(768)
        self.drop = nn.Dropout(0.5)

    def forward(self, image_features, cls_token, sep_token, img_tok_ids):
        # 変数定義
        batch_size = image_features.size(0) # バッチサイズを取得
        seq_length = image_features.size(1) + 2 # CLS + Pooling + SEP

        # CLS・SEPをバッチサイズ分コピー
        cls_batch = cls_token.expand(batch_size, -1, -1)
        # cls_batch.shape:  torch.Size([batch_size, 1, 768])
        sep_batch = sep_token.expand(batch_size, -1, -1) # -1はその次元のサイズを変更しないことを意味
        # sep_batch.shape:  torch.Size([bacth_size, 1, 768])
        # 画像ベクトルを 2048 -> 768 に次元圧縮
        image_features = self.image_embeddings(image_features)
        #image_features:  torch.Size([batch_size, 3, 768])
        # cls + image + sep
        image_token_embeddings = torch.cat([cls_batch, image_features, sep_batch], dim=1)
        # image_token_embeddings:  torch.Size([batch_size, 5, 768])
        # cls + pooling_size + sep = 5
        # position_ids
        position_ids = torch.arange(seq_length, dtype=torch.long).to(image_features.device)
        position_ids = position_ids.expand(batch_size, -1)
        position_embeddings = self.position_embeddings(position_ids)
        token_type_embeddings = self.token_type_embeddings(img_tok_ids)
        #image_token_embeddingsとtext_featuresを結合
        embeddings = image_token_embeddings + position_embeddings + token_type_embeddings

        # LayerNormalization
        embeddings = self.ln(embeddings)

        # DropOut
        embeddings = self.drop(embeddings)
        # embeddings:  [batch_size, 5+token_size, 768]
        return embeddings

class MultimodalBertEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        # bertモデルの定義
        self.bert_model = BertModel.from_pretrained(config.bert_model).to(config.device)
        self.bert_tokeniser = BertTokenizer.from_pretrained(config.bert_model)
        self.txt_embeddings = self.bert_model.embeddings # 文章ベクトル

        self.image_embeddings = ImageBertEmbeddings(config, self.txt_embeddings)

        self.encoder = self.bert_model.encoder
        self.pooler = self.bert_model.pooler

        # Resnet
        self.resnet_model = resnet152(weights=ResNet152_Weights.DEFAULT).to(config.device)
        self.resnet_features = nn.Sequential(*(list(self.resnet_model.children())[:-2]))
        self.custom_pool = nn.AdaptiveAvgPool2d((1, 3)) # カスタムプール

    def forward(self, images, text):
        # text_featuresの取得
        # cls_token, sep_tokenの取得
        encoded_input = self.bert_tokeniser(
            text,
            return_tensors='pt',  # PyTorchのテンソル形式で出力
            padding=True,         # 最長のものに合わせてパディング
            truncation=True,      # 最大長でトランケーション
            max_length=128        # 最大長を128トークンに設定
        )
        input_ids = encoded_input['input_ids'].to(config.device)
        attention_mask = encoded_input['attention_mask'].to(config.device)
        token_type_ids = encoded_input['token_type_ids'].to(config.device) if 'token_type_ids' in encoded_input else None
        # CLSトークンの取得
        cls_token = self.txt_embeddings.word_embeddings(torch.tensor([101]).to(config.device))
        # SEPトークンの取得
        sep_token = self.txt_embeddings.word_embeddings(torch.tensor([102]).to(config.device))
        outputs = self.bert_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                token_type_ids=token_type_ids
                )
        # 文章特徴量の取得
        text_features = outputs.last_hidden_state

        # 画像特徴量の取得
        images = images.to(config.device)
        output = self.resnet_features(images)                # モデルを通じて画像から特徴を抽出
        output = self.custom_pool(output)                    # カスタムプールを適用
        output = torch.flatten(output, start_dim=2)          # 特徴量をフラット化
        image_features = output.transpose(1, 2).contiguous() # 特徴量の次元を転置

        # 変数定義
        batch_size = images.size(0)
        image_seq_length = image_features.size(1) + 2

        text_features = text_features.to(config.device)

        # attention_maskの更新
        # 入力するattention_maskはtextにしか適用していないので
        # imageの方にも適用するよう更新する

        attention_mask = torch.cat(
            [
                torch.ones(batch_size, image_seq_length).long().to(config.device),
                attention_mask,
            ],
            dim=1,
        )

        # attention_maskの変形．1番目と2番目の次元を1つ増やし、[batch_size,1,1, seq_length]の形にする
        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)

        # torch.float32に型変換（次の処理でfloatが必要になる）
        extended_attention_mask = extended_attention_mask.to(
            dtype=next(self.parameters()).dtype
        )

        # Attentionを掛けない部分はマイナス無限大にしたいので、代わりに-10000を掛け算する
        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0

        #[batch_size, image_seq_length]の箱作り．ImageBertEmbeddingsの引数
        img_tok = (
            torch.LongTensor(batch_size, image_seq_length)
            .fill_(0)
            .to(config.device)
        )

        # この手前で，image_feature, cls_token, sep_tokenを取得する
        img_embed_out = self.image_embeddings(image_features, cls_token, sep_token, img_tok)

        # 画像ベクトルとテキストベクトルを連結
        encoder_input = torch.cat([img_embed_out, text_features], dim=1).to(config.device)

        # BERTエンコーダに入力
        encoded_layers = self.encoder(encoder_input, attention_mask=extended_attention_mask, return_dict=True)

        # clsベクトルを出力
        cls_vectors = encoded_layers.last_hidden_state[:, 0, :] # CLSトークンの取得

        return cls_vectors

# 分類器
class MMBTClf(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.enc = MultimodalBertEncoder(config)

        # Main
        self.ln1 = nn.LayerNorm(768)
        self.fc1 = nn.Linear(768, 384)
        self.dp1 = nn.Dropout(0.5)
        self.ln2 = nn.LayerNorm(384)
        self.fc2 = nn.Linear(384, 128)
        self.dp2 = nn.Dropout(0.3)
        self.fc3 = nn.Linear(128, config.n_classes) # 6392

        # マスク学習器
        self.answerlabel_head = nn.Sequential(
            nn.Linear(384, 5),
        )
        self.mask_head = nn.Sequential(
            nn.Linear(5, config.n_classes),
            nn.Sigmoid()
        )

        # unans_head
        self.unans_head = nn.Sequential(
            nn.Linear(768, 1),
            nn.Sigmoid()
        )

        # yesno_head
        self.yesno_head = nn.Sequential(
            nn.Linear(768, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, 3)
        )

        # color_head
        self.color_head = nn.Sequential(
            nn.Linear(768, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Linear(512, 7)
        )


    def forward(self, images, text):
        cls = self.enc(images, text)
        # cls.shape [batch_size, 768]

        #Main
        x = self.ln1(cls)
        x = self.fc1(x) # 768 -> 384
        x = F.relu(x)
        x = self.dp1(x)

        # Mask 5分類マスク
        label_feats = x
        answer_label = self.answerlabel_head(label_feats) # 384 -> 5
        mask = self.mask_head(answer_label) # 5 -> 6392

        x = self.ln2(x)
        x = self.fc2(x) # 384 -> 128
        x = F.relu(x)
        feats = self.dp2(x)
        vqa = self.fc3(feats) # 128 -> num_classes

        # 出力層直前の特徴量を抽出（IBFLossに利用）
        label_feats = torch.sum(torch.abs(label_feats), 1).reshape(-1, 1)
        feats = torch.sum(torch.abs(feats), 1).reshape(-1, 1)
        output = vqa * mask

        # sub task
        # unans
        unans_logits = self.unans_head(cls).squeeze(-1)

        # yes/no
        yesno_logits = self.yesno_head(cls)

        # color
        color_logits = self.color_head(cls)


        return output, feats, answer_label, label_feats, unans_logits, yesno_logits, color_logits

model = MMBTClf(config)
summary(model)

# 学習
def MMBT_train(model, dataloader, optimizer, criterion, device, IBFLoss=False):
    model.train()

    total_loss = 0
    total_acc = 0
    simple_acc = 0
    start = time.time()

    # sub_criterion
    unans_criterion = nn.BCELoss()
    yesno_criterion = nn.CrossEntropyLoss()
    color_criterion = nn.CrossEntropyLoss()

    for image, question, label, answers, mode_answer, unans_label, yesno_label, color_label in dataloader:
      optimizer.zero_grad()
      image = image.to(device)
      label = label.to(device)
      answers = answers.to(device)
      mode_answer = mode_answer.to(device)
      unans_label = unans_label.float().to(device)
      yesno_label = yesno_label.to(device)
      color_label = color_label.to(device)


      # 推論
      pred, features, pred_label, label_features, unans_logits, yesno_logits, color_logits = model(image, question)


      # 損失計算

      # Main
      if IBFLoss :
          loss_main = criterion(pred, mode_answer, features, config.n_classes) # Influence-BalancedFocalLoss
          loss_label = criterion(pred_label, label, label_features, 5) # Influence-BalancedFocalLoss
      else:
          loss_main = criterion(pred, mode_answer) #CrossEntropyLoss
          loss_label = criterion(pred_label, label) #CrossEntropyLoss
      # Sub
      loss_unans = unans_criterion(unans_logits, unans_label) # BCELoss
      loss_yesno = yesno_criterion(yesno_logits, yesno_label)
      loss_color = color_criterion(color_logits, color_label)

      loss = loss_main + loss_label + loss_unans + loss_yesno + loss_color
      loss.backward()
      optimizer.step()

      total_loss += loss.item()
      total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy
      simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy

    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start

batch_size = 10
model_save = True
submission = True

train_dataset = Train_Dataset(aug_df, image_dir=config.train_image_dir, class_map_path=config.class_mapping, transforms=transforms_aug, answer=True)
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=test_transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)
# loaderの中身
# image:       [batch_size, 3, 224, 224]
# question:    sentence
# answers:     [batch_size, 10]
# mode_answer: [batch_size, ]

epochs = 10
model = MMBTClf(config).to(config.device)
model.apply(initialize_weights)
optimizer = torch.optim.Adam(model.parameters(), lr=8e-4, weight_decay=5e-3) ## 場所に応じてlrを変更すること
criterion = nn.CrossEntropyLoss()

for epoch in tqdm(range(epochs)):
    torch.cuda.empty_cache()
    train_loss, train_acc, train_simple_acc, train_time = MMBT_train(model, train_loader, optimizer, criterion, config.device, IBFLoss=False)
    print(f"【{epoch + 1}/{epochs}】\n"
              f"train time: {train_time:.2f} [s]\n"
              f"train loss: {train_loss:.4f}\n"
              f"train acc: {train_acc:.4f}\n"
              f"train simple acc: {train_simple_acc:.4f}")


if model_save:
  # モデルの保存
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/model"
  save_path = os.path.join(save_dir, f"model_MMBT_{epochs}.pth")  # Driveに保存
  torch.save({
              "epoch": epoch,
              "model.state_dict": model.state_dict(),
              "optimizer_state_dict": optimizer.state_dict(),
              "loss": train_loss,
                }, save_path)

if submission:
  # 提出用ファイルの作成
  model.eval()
  submission = []
  with torch.no_grad():
      for image, question in tqdm(test_loader):
          image = image.to(device)

          pred, features, pred_label, label_features, unans_logits, yesno_logits, color_logits = model(image, question)
          pred = pred.argmax(1).cpu().item()
          submission.append(pred)

  submission = [train_dataset.idx2answer[id] for id in submission]
  submission = np.array(submission)
  #torch.save(model.state_dict(), "model02-2.pth")
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
  save_path = os.path.join(save_dir, "submission_MMBT.npy") # Driveの保存
  np.save(save_path, submission)

  # データフレームに変換し，CSV保存
  submission_df = pd.DataFrame(submission, columns=["answer"])
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
  save_path = os.path.join(save_dir, "submission_MMBT.csv") # Driveの保存  ######## モデル保存名注意！！ ########
  submission_df.to_csv(save_path, index=False)

print("done!")

# 追加学習IBFLoss
# 追加の学習数を指定
torch.cuda.empty_cache()
additional_epochs = 10
batch_size = 10
model_save = True
submission = True

train_dataset = Train_Dataset(aug_df, image_dir=config.train_image_dir, class_map_path=config.class_mapping, transforms=transforms_aug, answer=True)
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=test_transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)


# モデルの再定義
model = MMBTClf(config).to(config.device)
print(summary(model))
optimizer = torch.optim.Adam(model.parameters(), lr=8e-4, weight_decay=5e-3)
criterion = IB_FocalLoss(config)

######## 要変更箇所 ########
checkpoint_path = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/model/model_MMBT_10.pth"

# モデルとオプティマイザの状態を読み込む
checkpoint = torch.load(checkpoint_path)
model.load_state_dict(checkpoint["model.state_dict"])
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
start_epoch = checkpoint['epoch'] + 1
loss = checkpoint['loss']

print(f"Resuming training from epoch {start_epoch} with last recorded loss {loss:.4f}")


# 追加で学習するエポック数を設定
epochs = start_epoch + additional_epochs

# 学習
for epoch in tqdm(range(start_epoch, epochs)):
    torch.cuda.empty_cache()
    train_loss, train_acc, train_simple_acc, train_time = MMBT_train(model, train_loader, optimizer, criterion, config.device, IBFLoss=True)
    print(f"【{epoch + 1}/{epochs}】\n"
              f"train time: {train_time:.2f} [s]\n"
              f"train loss: {train_loss:.4f}\n"
              f"train acc: {train_acc:.4f}\n"
              f"train simple acc: {train_simple_acc:.4f}")

if model_save:
  # モデルの保存
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/model"
  save_path = os.path.join(save_dir, f"model_MMBT_{epochs}.pth")  # Driveに保存
  torch.save({
              "epoch": epoch,
              "model.state_dict": model.state_dict(),
              "optimizer_state_dict": optimizer.state_dict(),
              "loss": train_loss,
                }, save_path)

if submission:
  # 提出用ファイルの作成
  model.eval()
  submission = []
  with torch.no_grad():
      for image, question in tqdm(test_loader):
          image = image.to(device)

          pred, features, pred_label, label_features, unans_logits, yesno_logits, color_logits = model(image, question)
          pred = pred.argmax(1).cpu().item()
          submission.append(pred)

  submission = [train_dataset.idx2answer[id] for id in submission]
  submission = np.array(submission)
  #torch.save(model.state_dict(), "model02-2.pth")
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
  save_path = os.path.join(save_dir, "submission_MMBT.npy") # Driveの保存
  np.save(save_path, submission)

  # データフレームに変換し，CSV保存
  submission_df = pd.DataFrame(submission, columns=["answer"])
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
  save_path = os.path.join(save_dir, "submission_MMBT.csv") # Driveの保存  ######## モデル保存名注意！！ ########
  submission_df.to_csv(save_path, index=False)

print("done!")

del model
torch.cuda.empty_cache()

"""### ViLTモデル"""

# カスタムモデルクラスを定義
class CustomViltForQuestionAnswering(ViltForQuestionAnswering):
    def __init__(self, config):
        super().__init__(config)
        self.pooler_output = None
        self.classifier_input = None
        self.classifier_output = None

        # 新しい分類器を設定
        self.classifier = nn.Sequential(
          nn.Linear(in_features=768, out_features=1536, bias=True),
          nn.LayerNorm((1536,), eps=1e-05, elementwise_affine=True),
          nn.GELU(approximate='none'),
          nn.Linear(in_features=1536, out_features=6392, bias=True)
        )


        # classifierの各層の出力フックを設定
        self.classifier[0].register_forward_hook(self.get_classifier_input)
        self.classifier[-1].register_forward_hook(self.get_classifier_output)


    def get_classifier_input(self, module, input, output):
        self.classifier_input = input[0]

    def get_classifier_output(self, module, input, output):
        self.classifier_output = output

    def forward(self, *args, **kwargs):
        # 元のフォワードパスを実行
        outputs = super().forward(*args, **kwargs)

        return outputs

# カスタムモデルを初期化
custom_vilt_model = CustomViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa", ignore_mismatched_sizes=True)

# モデル定義
class ViLT_Clf(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.vilt = CustomViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa", ignore_mismatched_sizes=True)
        self.processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa", do_rescale=False)

        # Mask
        self.answerlabel_head = nn.Sequential(
            nn.Linear(768, 5),
        )
        self.mask_head = nn.Sequential(
            nn.Linear(5, 6392),
            nn.Sigmoid()
        )

        # unans_head
        self.unans_head = nn.Sequential(
            nn.Linear(768, 1),
            nn.Sigmoid()
        )
        # yesno_head
        self.yesno_head = nn.Sequential(
            nn.Linear(768, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Linear(256, 3)
        )
        # color_head
        self.color_head = nn.Sequential(
            nn.Linear(768, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Linear(256, 7)
        )


    def forward(self, image, question):
        # Viltモデルへ入力のためのprocessor
        inputs = self.processor(image, question, return_tensors="pt", truncation=True, padding="max_length")
        # scaling: OFF(transformでスケーリング済)
        inputs = {k: v.to(config.device) for k, v in inputs.items() if isinstance(v, torch.Tensor)} # deviceにあてる

        outputs = self.vilt(**inputs)
        #

        # sub task
        classifier_input_vectors = self.vilt.classifier_input # classifierへの入力ベクトル
                                                              # サブタスクの入力に用いる

        # mask
        answer_label = self.answerlabel_head(classifier_input_vectors) # 768 -> 5
        mask = self.mask_head(answer_label) # 5 -> 6392

        # unans
        unans_logits = self.unans_head(classifier_input_vectors).squeeze(-1)

        # yes/no
        yesno_logits = self.yesno_head(classifier_input_vectors)

        # color
        color_logits = self.color_head(classifier_input_vectors)

        outputs = outputs["logits"] * mask

        return outputs, answer_label, unans_logits, yesno_logits, color_logits

model = ViLT_Clf(config).to(config.device)
summary(model)

# 学習
def ViLT_train(model, dataloader, optimizer, criterion, device):
    model.train()

    total_loss = 0
    total_acc = 0
    simple_acc = 0
    start = time.time()

    # sub_criterion
    label_criterion = nn.CrossEntropyLoss()
    unans_criterion = nn.BCELoss()
    yesno_criterion = nn.CrossEntropyLoss()
    color_criterion = nn.CrossEntropyLoss()
    number_criterion = nn.CrossEntropyLoss()

    cnt = 0
    for image, question, label, answers, mode_answer, unans_label, yesno_label, color_label in dataloader:
        cnt += 1
        optimizer.zero_grad()
        #image = image.to(device)
        answers = answers.to(device)
        label = label.to(device)
        mode_answer = mode_answer.to(device)
        unans_label = unans_label.float().to(device)
        yesno_label = yesno_label.to(device)
        color_label = color_label.to(device)


        # 推論
        pred, pred_label, uans_logits, yes_no_logits, color_logits = model(image, question)

        log_pred = F.log_softmax(pred, dim=1)
        one_hot_targets = F.one_hot(mode_answer, num_classes=config.n_classes).float()

        loss_main = criterion(log_pred, one_hot_targets) # KLDivLoss
        loss_label = label_criterion(pred_label, label) # CrossEntropyLoss
        loss_unans = unans_criterion(uans_logits, unans_label) # BCELoss
        loss_yesno = yesno_criterion(yes_no_logits, yesno_label) # CrossEntropyLoss
        loss_color = color_criterion(color_logits, color_label) # CrossEntropyLoss

        # 損失計算
        loss = loss_main + loss_label + loss_unans + loss_yesno + loss_color
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy
        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy

        #if cnt == 50:
          #break

    return total_loss/len(dataloader), total_acc/len(dataloader), simple_acc/len(dataloader), time.time() - start

batch_size = 32
model_save = True
submission = False

train_dataset = Train_Dataset(aug_df, image_dir=config.train_image_dir, class_map_path=config.class_mapping, transforms=transforms_aug, answer=True)
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=test_transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)
# loaderの中身
# image:       [batch_size, 3, 224, 224]
# question:    sentence
# answers:     [batch_size, 10]
# mode_answer: [batch_size, ]

epochs = 5
model = ViLT_Clf(config).to(config.device)
# パラメータごとに学習率を指定
# ファインチューニング部分は低学習率
param_groups = [
    {'params': model.answerlabel_head.parameters(), 'lr': 3e-4},
    {'params': model.mask_head.parameters(), 'lr': 3e-4},
    {'params': model.unans_head.parameters(), 'lr': 3e-4},
    {'params': model.yesno_head.parameters(), 'lr': 3e-4},
    {'params': model.color_head.parameters(), 'lr': 3e-4},
    {'params': [param for name, param in model.named_parameters() if 'answerlabel_head' not in name and
                                                          'mask_head' not in name and
                                                          'unans_head' not in name and
                                                          'yesno_head' not in name and
                                                          'color_head' not in name], 'lr': 8e-5}  # その他のパラメータに異なる学習率を設定
]
optimizer = torch.optim.Adam(param_groups, weight_decay=5e-4)
criterion = nn.KLDivLoss(reduction="batchmean")

for epoch in tqdm(range(epochs)):
    train_loss, train_acc, train_simple_acc, train_time = ViLT_train(model, train_loader, optimizer, criterion, config.device)
    print(f"【{epoch + 1}/{epochs}】\n"
              f"train time: {train_time:.2f} [s]\n"
              f"train loss: {train_loss:.4f}\n"
              f"train acc: {train_acc:.4f}\n"
              f"train simple acc: {train_simple_acc:.4f}")


if model_save:
  # モデルの保存
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/model"
  save_path = os.path.join(save_dir, f"model_ViLT_{epochs}.pth")  # Driveに保存
  torch.save({
              "epoch": epoch,
              "model.state_dict": model.state_dict(),
              "optimizer_state_dict": optimizer.state_dict(),
              "loss": train_loss,
                }, save_path)

if submission:
  # 提出用ファイルの作成
  model.eval()
  submission = []
  with torch.no_grad():
      for image, question in tqdm(test_loader):

          pred, pred_label, uans_logits, yes_no_logits, color_logits = model(image, question)
          pred = pred.argmax(1).cpu().item()
          submission.append(pred)

  submission = [train_dataset.idx2answer[id] for id in submission]
  submission = np.array(submission)
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
  save_path = os.path.join(save_dir, "submission_ViLT.npy") # Driveの保存
  np.save(save_path, submission)

  # データフレームに変換し，CSV保存
  submission_df = pd.DataFrame(submission, columns=["answer"])
  save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
  save_path = os.path.join(save_dir, "submission_ViLT.csv") # Driveの保存  ######## モデル保存名注意！！ ########
  submission_df.to_csv(save_path, index=False)

print("done!")

del model
torch.cuda.empty_cache()

"""## Ensemble"""

# モデル定義
model_clip = MultiTask_Clf(config).to(config.device)
model_mmbt = MMBTClf(config).to(config.device)
model_ViLT = ViLT_Clf(config).to(config.device)


# 学習済みモデルの読み込み
checkpoint_path = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/model/model_MultiTask_20.pth"
checkpoint = torch.load(checkpoint_path)
model_clip.load_state_dict(checkpoint["model.state_dict"])
model_clip.eval()

# MMBT
checkpoint_path = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/model/model_MMBT_20.pth"
checkpoint = torch.load(checkpoint_path)
model_mmbt.load_state_dict(checkpoint["model.state_dict"])
model_mmbt.eval()

# ViLT
checkpoint_path = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/model/model_ViLT_5.pth"
checkpoint = torch.load(checkpoint_path)
model_ViLT.load_state_dict(checkpoint["model.state_dict"])
model_ViLT.eval()

batch_size=2
train_dataset = Train_Dataset(aug_df, image_dir=config.train_image_dir, class_map_path=config.class_mapping, transforms=transforms_aug, answer=True)

# 推論 #

# for MMBT
Test_transform = test_transform # 回転無し
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=Test_transform)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)
all_preds = []
with torch.no_grad():
  for image, question in tqdm(test_loader):
      image = image.to(device)

      pred, _, _, _, _, _, _ = model_mmbt(image, question)
      all_preds.append(pred)
  mmbt_pred_0 = torch.cat(all_preds, dim=0)

Test_transform = test_transform_rotate_90 # 90回転
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=Test_transform)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)
all_preds = []
with torch.no_grad():
  for image, question in tqdm(test_loader):
      image = image.to(device)

      pred, _, _, _, _, _, _ = model_mmbt(image, question)
      all_preds.append(pred)
  mmbt_pred_90 = torch.cat(all_preds, dim=0)

Test_transform = test_transform_rotate_270 # 270回転
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=Test_transform)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)
all_preds = []
with torch.no_grad():
  for image, question in tqdm(test_loader):
      image = image.to(device)

      pred, _, _, _, _, _, _ = model_mmbt(image, question)
      all_preds.append(pred)
  mmbt_pred_270 = torch.cat(all_preds, dim=0)


# for ViLT
Test_transform = test_transform # 回転無し
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=Test_transform)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)
all_preds = []
with torch.no_grad():
  for image, question in tqdm(test_loader):
      image = image.to(device)

      pred, _, _, _, _ = model_ViLT(image, question)
      all_preds.append(pred)
  vilt_pred_0 = torch.cat(all_preds, dim=0)

Test_transform = test_transform_rotate_90 # 90度回転
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=Test_transform)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)
all_preds = []
with torch.no_grad():
  for image, question in tqdm(test_loader):
      image = image.to(device)

      pred, _, _, _, _ = model_ViLT(image, question)
      all_preds.append(pred)
  vilt_pred_90 = torch.cat(all_preds, dim=0)

Test_transform = test_transform_rotate_270 # 270度回転
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=Test_transform)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)
all_preds = []
with torch.no_grad():
  for image, question in tqdm(test_loader):
      image = image.to(device)

      pred, _, _, _, _ = model_ViLT(image, question)
      all_preds.append(pred)
  vilt_pred_270 = torch.cat(all_preds, dim=0)


# for CLIP
train_dataset = Train_Dataset(aug_df, image_dir=config.train_image_dir, class_map_path=config.class_mapping, transforms=None, preprocesses=preprocess_aug, answer=True)

Test_preprocess = test_preprocess # 回転なし
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=None, preprocess=Test_preprocess)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)
all_preds = []
with torch.no_grad():
  for image, question in tqdm(test_loader):
    image = image.to(device)
    text = clip.tokenize(question, truncate=True).to(device) # 質問文をIDに変換

    # embedding
    text_features = clip_model.encode_text(text)
    # text_features.shape  [batch_size, 768]
    image_features = clip_model.encode_image(image)
    # image_features.shape  [batch_size, 768]
    # 推論
    with autocast():
      pred, features, pred_label, label_features, unans_logits, yesno_logits, color_logits = model_clip(image_features, text_features)
    pred = pred.float()
    all_preds.append(pred)
  clip_pred_0 = torch.cat(all_preds, dim=0)

Test_preprocess = test_preprocess_rotate_90 # 90度回転
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=None, preprocess=Test_preprocess)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)
all_preds = []
with torch.no_grad():
  for image, question in tqdm(test_loader):
    image = image.to(device)
    text = clip.tokenize(question, truncate=True).to(device) # 質問文をIDに変換

    # embedding
    text_features = clip_model.encode_text(text)
    # text_features.shape  [batch_size, 768]
    image_features = clip_model.encode_image(image)
    # image_features.shape  [batch_size, 768]
    # 推論
    with autocast():
      pred, features, pred_label, label_features, unans_logits, yesno_logits, color_logits = model_clip(image_features, text_features)
    pred = pred.float()
    all_preds.append(pred)
  clip_pred_90 = torch.cat(all_preds, dim=0)

Test_preprocess = test_preprocess_rotate_270 # 270度回転
test_dataset = Test_Dataset(df_path=config.valid_df_dir, image_dir=config.valid_image_dir, class_map_path=config.class_mapping, transform=None, preprocess=Test_preprocess)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
test_dataset.update_dict(train_dataset)
all_preds = []
with torch.no_grad():
  for image, question in tqdm(test_loader):
    image = image.to(device)
    text = clip.tokenize(question, truncate=True).to(device) # 質問文をIDに変換

    # embedding
    text_features = clip_model.encode_text(text)
    # text_features.shape  [batch_size, 768]
    image_features = clip_model.encode_image(image)
    # image_features.shape  [batch_size, 768]
    # 推論
    with autocast():
      pred, features, pred_label, label_features, unans_logits, yesno_logits, color_logits = model_clip(image_features, text_features)
    pred = pred.float()
    all_preds.append(pred)
  clip_pred_270 = torch.cat(all_preds, dim=0)


submissiont = []

submission_preds = 2.5*(1.5*clip_pred_0 + clip_pred_90 + clip_pred_270) + 0.03*(mmbt_pred_0 + mmbt_pred_90 + mmbt_pred_270) + 0.18*(vilt_pred_0 + vilt_pred_90 + vilt_pred_270)

submission_labels = torch.argmax(submission_preds, dim=1).cpu().numpy()

submission = [train_dataset.idx2answer[id] for id in submission_labels]

save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
save_path = os.path.join(save_dir, "submission_ensemble.npy") # Driveの保存
np.save(save_path, submission)

# データフレームに変換し，CSV保存
submission_df = pd.DataFrame(submission, columns=["answer"])
save_dir = "/content/drive/MyDrive/大学/13_【DL基礎】東京大学松尾研/00_final/dl_lecture_competition_pub/checkpoint/submission"
save_path = os.path.join(save_dir, "submission_ensemble.csv") # Driveの保存  ######## モデル保存名注意！！ ########
submission_df.to_csv(save_path, index=False)
print("done!")